# 代码学习

## FLAT: Chinese NER Using Flat-Lattice Transformer

### 模型部分：Lattice_Transformer_SeqLabel

编码：Transformer_Encoder

层数：1层Transformer_Encoder_Layer

**attention结构：MultiHead_Attention_Lattice_rel_save_gpumm**

前馈结构：Positionwise_FeedForward

**位置embedding：Four_Pos_Fusion_Embedding**

预/后处理：Layer_Process

### 训练部分

[fastNLP trainer源码](https://fastnlp.readthedocs.io/zh/latest/_modules/fastNLP/core/trainer.html#Trainer)

